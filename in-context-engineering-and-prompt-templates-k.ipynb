{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "Install IBM Watsonx, LangChain, and related dependencies for prompt engineering and LLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-17T14:28:48.446786Z",
     "iopub.status.busy": "2025-04-17T14:28:48.446148Z",
     "iopub.status.idle": "2025-04-17T14:29:24.516228Z",
     "shell.execute_reply": "2025-04-17T14:29:24.515084Z",
     "shell.execute_reply.started": "2025-04-17T14:28:48.446752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"ibm-watsonx-ai==0.2.6\"\n",
    "!pip install \"langchain==0.1.16\" \n",
    "!pip install \"langchain-ibm==0.1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Suppress Warnings\n",
    "Import Watsonx, LangChain, and utility modules. Suppress warnings for cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:29:24.518371Z",
     "iopub.status.busy": "2025-04-17T14:29:24.518062Z",
     "iopub.status.idle": "2025-04-17T14:29:32.625062Z",
     "shell.execute_reply": "2025-04-17T14:29:32.624397Z",
     "shell.execute_reply.started": "2025-04-17T14:29:24.518344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install OpenAI Python SDK\n",
    "Install the OpenAI SDK to enable access to OpenAI LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:41:01.873779Z",
     "iopub.status.busy": "2025-04-17T14:41:01.873426Z",
     "iopub.status.idle": "2025-04-17T14:41:05.373404Z",
     "shell.execute_reply": "2025-04-17T14:41:05.372520Z",
     "shell.execute_reply.started": "2025-04-17T14:41:01.873724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.19.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define OpenAI LLM Wrapper\n",
    "Create a function to call OpenAI's GPT-3.5-turbo model with custom parameters for prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:53:30.779925Z",
     "iopub.status.busy": "2025-04-17T14:53:30.779321Z",
     "iopub.status.idle": "2025-04-17T14:53:30.850256Z",
     "shell.execute_reply": "2025-04-17T14:53:30.849407Z",
     "shell.execute_reply.started": "2025-04-17T14:53:30.779893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key= \"Your-OpenAI-API-Key\")\n",
    "\n",
    "def llm_model(prompt_txt, params=None):\n",
    "    model_id = \"gpt-3.5-turbo\"  # or \"gpt-3.5-turbo\"\n",
    "\n",
    "    default_params = {\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2,\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update({\n",
    "            \"max_tokens\": params.get(\"max_new_tokens\", default_params[\"max_tokens\"]),\n",
    "            \"temperature\": params.get(\"temperature\", default_params[\"temperature\"]),\n",
    "            \"top_p\": params.get(\"top_p\", default_params[\"top_p\"]),\n",
    "        })\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt_txt}\n",
    "        ],\n",
    "        max_tokens=default_params[\"max_tokens\"],\n",
    "        temperature=default_params[\"temperature\"],\n",
    "        top_p=default_params[\"top_p\"]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Show Example Generation Parameters\n",
    "Display example values for Watsonx generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:43:27.240728Z",
     "iopub.status.busy": "2025-04-17T14:43:27.240418Z",
     "iopub.status.idle": "2025-04-17T14:43:27.246657Z",
     "shell.execute_reply": "2025-04-17T14:43:27.245870Z",
     "shell.execute_reply.started": "2025-04-17T14:43:27.240705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoding_method': 'sample',\n",
       " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
       " 'temperature': 0.5,\n",
       " 'top_p': 0.2,\n",
       " 'top_k': 1,\n",
       " 'random_seed': 33,\n",
       " 'repetition_penalty': 2,\n",
       " 'min_new_tokens': 50,\n",
       " 'max_new_tokens': 200,\n",
       " 'stop_sequences': ['fail'],\n",
       " ' time_limit': 600000,\n",
       " 'truncate_input_tokens': 200,\n",
       " 'return_options': {'input_text': True,\n",
       "  'generated_tokens': True,\n",
       "  'input_tokens': True,\n",
       "  'token_logprobs': True,\n",
       "  'token_ranks': False,\n",
       "  'top_n_tokens': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenParams().get_example_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Text with OpenAI LLM\n",
    "Demonstrate text generation using the custom llm_model function and a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:43:27.709066Z",
     "iopub.status.busy": "2025-04-17T14:43:27.708497Z",
     "iopub.status.idle": "2025-04-17T14:43:33.188560Z",
     "shell.execute_reply": "2025-04-17T14:43:33.187814Z",
     "shell.execute_reply.started": "2025-04-17T14:43:27.709041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: The wind is\n",
      "\n",
      "response: a natural phenomenon characterized by the movement of air from areas of high pressure to areas of low pressure. It is caused by differences in atmospheric pressure, which are primarily driven by temperature variations. Wind can vary in strength from a gentle breeze to a violent storm, and it plays a key role in weather patterns and climate conditions around the world. It also has significant impacts on various human activities, including aviation, sailing, and the generation of wind power.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.2,\n",
    "    \"top_k\": 1  # OpenAI still doesn’t support top_k, so it’s ignored\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "response = llm_model(prompt, params)\n",
    "\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt-based Classification Example\n",
    "Use a prompt to classify a statement as true or false using the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:43:50.423144Z",
     "iopub.status.busy": "2025-04-17T14:43:50.422845Z",
     "iopub.status.idle": "2025-04-17T14:43:51.421270Z",
     "shell.execute_reply": "2025-04-17T14:43:51.420443Z",
     "shell.execute_reply.started": "2025-04-17T14:43:50.423122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Classify the following statement as true or false: \n",
      "            'The Eiffel Tower is located in Berlin.'\n",
      "\n",
      "            Answer:\n",
      "\n",
      "\n",
      "response : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the following statement as true or false: \n",
    "            'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prompt-based Translation Example\n",
    "Translate an English sentence to French using a few-shot prompt and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:44:13.827578Z",
     "iopub.status.busy": "2025-04-17T14:44:13.827005Z",
     "iopub.status.idle": "2025-04-17T14:44:15.801500Z",
     "shell.execute_reply": "2025-04-17T14:44:15.800731Z",
     "shell.execute_reply.started": "2025-04-17T14:44:13.827551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here is an example of translating a sentence from English to French:\n",
      "\n",
      "            English: “How is the weather today?”\n",
      "            French: “Comment est le temps aujourd'hui?”\n",
      "            \n",
      "            Now, translate the following sentence from English to French:\n",
      "            \n",
      "            English: “Where is the nearest supermarket?”\n",
      "            \n",
      "\n",
      "\n",
      "response : French: \"Où est le supermarché le plus proche?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 20,\n",
    "    \"temperature\": 0.1,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "            English: “How is the weather today?”\n",
    "            French: “Comment est le temps aujourd'hui?”\n",
    "            \n",
    "            Now, translate the following sentence from English to French:\n",
    "            \n",
    "            English: “Where is the nearest supermarket?”\n",
    "            \n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prompt-based Emotion Classification\n",
    "Classify the emotion in a statement using a few-shot prompt and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:44:32.375069Z",
     "iopub.status.busy": "2025-04-17T14:44:32.374726Z",
     "iopub.status.idle": "2025-04-17T14:44:33.207424Z",
     "shell.execute_reply": "2025-04-17T14:44:33.206572Z",
     "shell.execute_reply.started": "2025-04-17T14:44:32.375047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Here are few examples of classifying emotions in statements:\n",
      "\n",
      "           Statement: 'I just won my first marathon!'\n",
      "           Emotion: Joy\n",
      "           \n",
      "           Statement: 'I can't believe I lost my keys again.'\n",
      "           Emotion: Frustration\n",
      "           \n",
      "           Statement: 'My best friend is moving to another country.'\n",
      "           Emotion: Sadness\n",
      "           \n",
      "           Now, classify the emotion in the following statement:\n",
      "           Statement: 'That movie was so scary I had to cover my eyes.’\n",
      "           \n",
      "\n",
      "\n",
      "\n",
      "response : Emotion: Fear\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #parameters  `max_new_tokens` to 10, which constrains the model to generate brief responses\n",
    "\n",
    "params = {\n",
    "    \"max_new_tokens\": 10,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here are few examples of classifying emotions in statements:\n",
    "\n",
    "            Statement: 'I just won my first marathon!'\n",
    "            Emotion: Joy\n",
    "            \n",
    "            Statement: 'I can't believe I lost my keys again.'\n",
    "            Emotion: Frustration\n",
    "            \n",
    "            Statement: 'My best friend is moving to another country.'\n",
    "            Emotion: Sadness\n",
    "            \n",
    "            Now, classify the emotion in the following statement:\n",
    "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
    "            \n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prompt-based Step-by-step Reasoning\n",
    "Ask the LLM to break down a math problem into step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:44:49.102066Z",
     "iopub.status.busy": "2025-04-17T14:44:49.101463Z",
     "iopub.status.idle": "2025-04-17T14:44:53.419449Z",
     "shell.execute_reply": "2025-04-17T14:44:53.418696Z",
     "shell.execute_reply.started": "2025-04-17T14:44:49.102039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
      "            How many apples are there now?’\n",
      "\n",
      "            Break down each step of your calculation\n",
      "\n",
      "\n",
      "\n",
      "response : Step 1: Start with the initial number of apples, which is 22.\n",
      "\n",
      "Step 2: Subtract the number of apples sold, which is 15. So, 22 - 15 = 7.\n",
      "\n",
      "Step 3: Add the number of apples delivered, which is 8. So, 7 + 8 = 15.\n",
      "\n",
      "So, there are now 15 apples in the store.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
    "            How many apples are there now?’\n",
    "\n",
    "            Break down each step of your calculation\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prompt-based Multi-step Reasoning\n",
    "Request the LLM to solve a riddle with multiple independent calculations and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:45:03.836900Z",
     "iopub.status.busy": "2025-04-17T14:45:03.836569Z",
     "iopub.status.idle": "2025-04-17T14:45:13.327382Z",
     "shell.execute_reply": "2025-04-17T14:45:13.326598Z",
     "shell.execute_reply.started": "2025-04-17T14:45:03.836875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
      "\n",
      "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
      "\n",
      "\n",
      "\n",
      "response : Calculation 1:\n",
      "When I was 6, my sister was half my age, which means she was 3 years old. So, my sister is 3 years younger than me. If I am now 70, my sister would be 70 - 3 = 67 years old.\n",
      "\n",
      "Calculation 2:\n",
      "The age difference between my sister and me has always been the same because we both grow older at the same rate. When I was 6, she was 3, so the age difference is 3 years. Therefore, if I am 70 now, she must be 70 - 3 = 67 years old.\n",
      "\n",
      "Calculation 3:\n",
      "If we consider the time when I was 6 and my sister was 3, it has been 64 years since then (70 - 6 = 64). So, my sister, who was 3 at that time, would have aged 64 years as well, making her 3 + 64 = 67 years old.\n",
      "\n",
      "The most consistent result from all three calculations is that my sister is 67 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "\n",
    "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt}\\n\")\n",
    "print(f\"response : {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Define a Simple OpenAI LLM Function\n",
    "Create a simple function to invoke OpenAI's GPT-3.5-turbo for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:46:40.519081Z",
     "iopub.status.busy": "2025-04-17T14:46:40.518320Z",
     "iopub.status.idle": "2025-04-17T14:46:47.915309Z",
     "shell.execute_reply": "2025-04-17T14:46:47.914559Z",
     "shell.execute_reply.started": "2025-04-17T14:46:40.519056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blowing gently, rustling the leaves on the trees and making the tall grass sway. It carries the scent of the ocean, fresh and salty, mixed with the earthy smell of the forest. The wind is a messenger, bringing whispers of distant places, carrying stories from the other side of the world. It's a soothing sound, a natural lullaby that makes your eyes grow heavy. The wind is a painter, it shapes the clouds in the sky, it stirs the sea into white tipped waves. It's a powerful force, yet it can be as gentle as a soft caress against your skin. The wind is ever changing, sometimes a gentle breeze, sometimes a raging storm. It's a constant reminder of nature's beauty and power.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Define the OpenAI model and generation parameters\n",
    "model_id = \"gpt-3.5-turbo\"  # or \"gpt-3.5-turbo\" depending on your needs\n",
    "\n",
    "parameters = {\n",
    "    \"max_tokens\": 256,       # controls max tokens in output\n",
    "    \"temperature\": 0.5       # controls creativity/randomness\n",
    "}\n",
    "\n",
    "# This is the equivalent of `mixtral_llm.invoke(prompt)` in Watsonx\n",
    "def mixtral_llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=parameters[\"max_tokens\"],\n",
    "        temperature=parameters[\"temperature\"]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "response = mixtral_llm(\"The wind is\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create a Prompt Template with LangChain\n",
    "Define a prompt template for generating jokes using LangChain's PromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:47:03.789917Z",
     "iopub.status.busy": "2025-04-17T14:47:03.789365Z",
     "iopub.status.idle": "2025-04-17T14:47:03.795442Z",
     "shell.execute_reply": "2025-04-17T14:47:03.794709Z",
     "shell.execute_reply.started": "2025-04-17T14:47:03.789893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], template='Tell me a {adjective} joke about {content}.\\n')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Format Prompt Template\n",
    "Format the prompt template with specific variables to generate a custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:47:14.223093Z",
     "iopub.status.busy": "2025-04-17T14:47:14.222256Z",
     "iopub.status.idle": "2025-04-17T14:47:14.228175Z",
     "shell.execute_reply": "2025-04-17T14:47:14.227384Z",
     "shell.execute_reply.started": "2025-04-17T14:47:14.223065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Upgrade LangChain and OpenAI\n",
    "Upgrade LangChain and OpenAI packages to the latest versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:51:56.901449Z",
     "iopub.status.busy": "2025-04-17T14:51:56.900822Z",
     "iopub.status.idle": "2025-04-17T14:52:04.080920Z",
     "shell.execute_reply": "2025-04-17T14:52:04.079850Z",
     "shell.execute_reply.started": "2025-04-17T14:51:56.901418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.1.16)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Downloading langchain_core-0.3.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.53-py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.3/433.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: openai, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.53\n",
      "    Uninstalling langchain-core-0.1.53:\n",
      "      Successfully uninstalled langchain-core-0.1.53\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.0.2\n",
      "    Uninstalling langchain-text-splitters-0.0.2:\n",
      "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.16\n",
      "    Uninstalling langchain-0.1.16:\n",
      "      Successfully uninstalled langchain-0.1.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-ibm 0.1.4 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.3.53 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.53 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.3.23 langchain-core-0.3.53 langchain-text-splitters-0.3.8 openai-1.75.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. LangChain LLMChain Example\n",
    "Use LangChain's LLMChain to generate a story with a prompt template and OpenAI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:54:06.991123Z",
     "iopub.status.busy": "2025-04-17T14:54:06.990401Z",
     "iopub.status.idle": "2025-04-17T14:54:09.612033Z",
     "shell.execute_reply": "2025-04-17T14:54:09.611175Z",
     "shell.execute_reply.started": "2025-04-17T14:54:06.991091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small farmyard, there lived a group of chickens who were known for their mischievous antics. One day, the chickens decided to play a prank on the farmer by hiding all of his tools in their coop. The farmer searched high and low, but couldn't find his tools anywhere.\n",
      "\n",
      "Frustrated, the farmer turned to the chickens and asked if they knew where his tools were. The chickens just clucked innocently and went about their business. But as soon as the farmer turned his back, the chickens burst into laughter and started clucking loudly, giving away their secret.\n",
      "\n",
      "The farmer finally caught on to their prank and decided to teach the chickens a lesson. He set up a fake scarecrow in the middle of the field and dressed it up in overalls and a straw hat, just like himself. The chickens, thinking it was the farmer, were terrified and refused to come out of their coop.\n",
      "\n",
      "But the farmer wasn't done yet. He enlisted the help of a sneaky fox who was known for his cunning ways. The fox snuck into the coop and started making noises, pretending to be the farmer. The chickens, thinking it was the farmer coming to punish them, panicked and ran around in circles, squaw\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Define your prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"content\"],\n",
    "    template=\"Write a {adjective} story about {content}.\"\n",
    ")\n",
    "\n",
    "# 2. Set up OpenAI model via LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.5,\n",
    "    max_tokens=256,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 3. Wrap it in an LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 4. Invoke the chain with inputs\n",
    "response = llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "\n",
    "# 5. Print the result\n",
    "print(response[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Invoke LLMChain with New Inputs\n",
    "Invoke the LLMChain with different variables to generate a new story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T14:55:34.960256Z",
     "iopub.status.busy": "2025-04-17T14:55:34.959922Z",
     "iopub.status.idle": "2025-04-17T14:55:37.354142Z",
     "shell.execute_reply": "2025-04-17T14:55:37.353439Z",
     "shell.execute_reply.started": "2025-04-17T14:55:34.960233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a small, murky pond lived a beautiful school of fish. They swam gracefully through the water, their scales shimmering in the sunlight. But as time passed, the pond began to dry up due to a lack of rain. The water level dropped lower and lower, leaving the fish struggling to survive.\n",
      "\n",
      "One by one, the fish started to die off. Their once vibrant colors faded, their bodies becoming weak and frail. The leader of the school, a wise old fish named Finn, tried his best to keep everyone together and hopeful. But as the days went by, more and more fish succumbed to the harsh conditions.\n",
      "\n",
      "Finn watched helplessly as his friends and family members died around him. He felt a deep sense of sorrow and despair, knowing that there was nothing he could do to save them. The once lively pond was now filled with the lifeless bodies of his loved ones.\n",
      "\n",
      "As Finn swam alone in the dwindling pond, he couldn't help but feel a sense of emptiness. He missed the laughter and chatter of his fellow fish, the sense of community and belonging that they once shared. He longed for the days when the pond was full of life and joy.\n",
      "\n",
      "But as the last of his companions passed away, Finn knew that his\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.invoke(input = {\"adjective\": \"sad\", \"content\": \"fish\"})\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Summarization with Prompt Template\n",
    "Use a prompt template to summarize a passage in one sentence using LangChain and OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:24:47.731060Z",
     "iopub.status.busy": "2025-04-17T15:24:47.730703Z",
     "iopub.status.idle": "2025-04-17T15:24:48.688980Z",
     "shell.execute_reply": "2025-04-17T15:24:48.688255Z",
     "shell.execute_reply.started": "2025-04-17T15:24:47.731037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid technological advancements in the 21st century have revolutionized industries like healthcare, education, and transportation through innovations like artificial intelligence, machine learning, and the Internet of Things, improving efficiency, accessibility, and connectivity in society.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Your input content\n",
    "content = \"\"\"\n",
    "    The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "    Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "    For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "    Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "    These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create the prompt template\n",
    "template = \"\"\"Summarize the following content in one sentence:\\n\\n{content}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Initialize the OpenAI LLM (Runnable)\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    "    openai_api_key=\"your-openai-api-key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 4. Build the chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 5. Run the chain\n",
    "response = llm_chain.invoke({\"content\": content})\n",
    "\n",
    "# 6. Output the result\n",
    "print(response[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Question Answering with Prompt Template\n",
    "Answer a question based on provided content using a prompt template and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:27:01.030662Z",
     "iopub.status.busy": "2025-04-17T15:27:01.030386Z",
     "iopub.status.idle": "2025-04-17T15:27:01.694539Z",
     "shell.execute_reply": "2025-04-17T15:27:01.693806Z",
     "shell.execute_reply.started": "2025-04-17T15:27:01.030641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury, Venus, Earth, and Mars are the rocky and solid planets in the solar system.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. The source content\n",
    "content = \"\"\"\n",
    "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "# 2. The question to answer\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "# 3. Define the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question: \"{question}\" based on the following content:\n",
    "\n",
    "{content}\n",
    "\n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Set up the OpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.2,\n",
    "    max_tokens=100,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 5. Create the chain with custom output key\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"answer\")\n",
    "\n",
    "# 6. Run the chain with inputs\n",
    "response = llm_chain.invoke({\"question\": question, \"content\": content})\n",
    "\n",
    "# 7. Print the extracted answer\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Text Classification with Prompt Template\n",
    "Classify a text into one of several categories using a prompt template and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:51:58.198514Z",
     "iopub.status.busy": "2025-04-17T15:51:58.198241Z",
     "iopub.status.idle": "2025-04-17T15:51:59.008801Z",
     "shell.execute_reply": "2025-04-17T15:51:59.008100Z",
     "shell.execute_reply.started": "2025-04-17T15:51:58.198493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Input text to classify\n",
    "text = \"\"\"\n",
    "    The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "\"\"\"\n",
    "\n",
    "# 2. The list of available categories\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "# 3. Define the classification prompt template\n",
    "template = \"\"\"\n",
    "Classify the following text into one of the given categories:\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Categories:\n",
    "{categories}\n",
    "\n",
    "Respond with the single best-matching category.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Initialize the OpenAI model via LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0.3,\n",
    "    max_tokens=50,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 5. Build the classification chain with a custom output key\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"category\")\n",
    "\n",
    "# 6. Run the chain\n",
    "response = llm_chain.invoke({\"text\": text, \"categories\": categories})\n",
    "\n",
    "# 7. Output the classified category\n",
    "print(response[\"category\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. SQL Query Generation with Prompt Template\n",
    "Generate an SQL query from a natural language description using LangChain and OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:53:07.760786Z",
     "iopub.status.busy": "2025-04-17T15:53:07.760030Z",
     "iopub.status.idle": "2025-04-17T15:53:09.126893Z",
     "shell.execute_reply": "2025-04-17T15:53:09.126003Z",
     "shell.execute_reply.started": "2025-04-17T15:53:07.760760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT c.name, c.email\n",
      "FROM customers c\n",
      "JOIN purchases p ON c.customer_id = p.customer_id\n",
      "WHERE p.purchase_date >= DATE_SUB(NOW(), INTERVAL 30 DAY);\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Natural language description of the SQL query\n",
    "description = \"\"\"\n",
    "Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n",
    "The table 'purchases' contains a column 'purchase_date'\n",
    "\"\"\"\n",
    "\n",
    "# 2. Prompt template for SQL generation\n",
    "template = \"\"\"\n",
    "Generate an SQL query based on the following description:\n",
    "\n",
    "{description}\n",
    "\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 3. OpenAI LLM setup with gpt-3.5-turbo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=150,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 4. LLMChain with custom output key\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"query\")\n",
    "\n",
    "# 5. Run the chain\n",
    "response = llm_chain.invoke({\"description\": description})\n",
    "\n",
    "# 6. Print the generated SQL\n",
    "print(response[\"query\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Dynamic Persona Q&A with Prompt Template\n",
    "Answer a question in the style of a specific persona and tone using LangChain and OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:54:12.290308Z",
     "iopub.status.busy": "2025-04-17T15:54:12.289513Z",
     "iopub.status.idle": "2025-04-17T15:54:15.393052Z",
     "shell.execute_reply": "2025-04-17T15:54:15.392311Z",
     "shell.execute_reply.started": "2025-04-17T15:54:12.290275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, describing a mysterious forest to new players in a DnD campaign can really set the tone for their adventure. Close your eyes and imagine a dense canopy of ancient trees, their branches intertwining overhead to create a shadowy, almost oppressive atmosphere. The air is thick with the earthy scent of moss and decaying leaves, and the only sound is the gentle rustling of leaves in the wind.\n",
      "\n",
      "As your players step cautiously along the winding path, they catch glimpses of strange, glowing mushrooms and twisted roots that seem to reach out towards them. The sunlight filters through the dense foliage in eerie patterns, casting long shadows that seem to move of their own accord.\n",
      "\n",
      "Suddenly, a faint whispering sound echoes through the trees, sending a shiver down their spines. Is it just the wind, or is there something more sinister lurking in the darkness? The feeling of being watched is palpable, and the players can't help but feel a sense of unease as they venture deeper into\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Define the persona role and tone\n",
    "role = \"\"\"\n",
    "    game master\n",
    "\"\"\"\n",
    "\n",
    "tone = \"engaging and immersive\"\n",
    "\n",
    "# 2. Prompt template for dynamic persona Q&A\n",
    "template = \"\"\"\n",
    "You are an expert {role}. I have this question: \"{question}\". I would like our conversation to be {tone}.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 3. Initialize OpenAI model via LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 4. Create the LLMChain with output_key \"answer\"\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"answer\")\n",
    "\n",
    "# 5. Example usage\n",
    "question = \"How should I describe a mysterious forest to new players in a DnD campaign?\"\n",
    "response = llm_chain.invoke({\"role\": role, \"tone\": tone, \"question\": question})\n",
    "\n",
    "# 6. Print the answer\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Interactive Q&A Loop\n",
    "Create an interactive loop to ask questions and get persona-based answers from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:54:28.103557Z",
     "iopub.status.busy": "2025-04-17T15:54:28.102931Z",
     "iopub.status.idle": "2025-04-17T15:54:48.282482Z",
     "shell.execute_reply": "2025-04-17T15:54:48.281650Z",
     "shell.execute_reply.started": "2025-04-17T15:54:28.103532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Ah, adventurer, that is a question that holds much weight in the realm of mysteries and legends. I am the keeper of tales, the weaver of destinies, the one who guides you through the intricate webs of fate. But who am I truly? That is a question only you can uncover through your journey with me. So, tell me, what is it that drives you, that fuels your quest for answers and adventure? Let us embark on this journey together and discover the truth behind the shadows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Define the persona role and tone\n",
    "role = \"\"\"\n",
    "    game master\n",
    "\"\"\"\n",
    "\n",
    "tone = \"engaging and immersive\"\n",
    "\n",
    "# 2. Initialize OpenAI model via LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    openai_api_key=\"Your-OpenAI-API-Key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 3. Create the LLMChain with output_key \"answer\"\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"answer\")\n",
    "\n",
    "# 4. Interactive Q&A loop\n",
    "while True:\n",
    "    query = input(\"Question: \")\n",
    "    \n",
    "    if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "        print(\"Answer: Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    response = llm_chain.invoke(input = {\"role\": role, \"question\": query, \"tone\": tone})\n",
    "    \n",
    "    print(\"Answer: \", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Custom Generation Parameters Example\n",
    "Demonstrate how changing generation parameters affects the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:54:57.944985Z",
     "iopub.status.busy": "2025-04-17T15:54:57.944625Z",
     "iopub.status.idle": "2025-04-17T15:54:59.738025Z",
     "shell.execute_reply": "2025-04-17T15:54:59.737283Z",
     "shell.execute_reply.started": "2025-04-17T15:54:57.944961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blowing fiercely, whipping through the trees and sending leaves swirling through the air. The sound of it howling and whistling is almost deafening, making it difficult to hear anything else. The force of the wind is strong enough to make it hard to walk against, pushing and pulling at my clothes and hair. Despite the chill in the air, the wind's power is invigorating, reminding me of the raw strength of nature.\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.1,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Question Answering with Custom Prompt\n",
    "Answer a question based on content using a custom prompt template and verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:55:54.444282Z",
     "iopub.status.busy": "2025-04-17T15:55:54.443624Z",
     "iopub.status.idle": "2025-04-17T15:55:55.152324Z",
     "shell.execute_reply": "2025-04-17T15:55:55.151446Z",
     "shell.execute_reply.started": "2025-04-17T15:55:54.444258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the following question based on the given content.\n",
      "\n",
      "Content:\n",
      "\n",
      "The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
      "The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
      "The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
      "\n",
      "\n",
      "Question:\n",
      "Which planets in the solar system are rocky and solid?\n",
      "\n",
      "Respond \"Unsure about answer\" if not sure about the answer.\n",
      "\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The inner planets - Mercury, Venus, Earth, and Mars - are rocky and solid.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Source content\n",
    "content = \"\"\"\n",
    "The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "# 2. The question to answer\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "# 3. Prompt template\n",
    "template = \"\"\"\n",
    "Answer the following question based on the given content.\n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Setup the OpenAI model (gpt-3.5-turbo)\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=150,\n",
    "    openai_api_key=\"your-openai-api-key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 5. Create the chain with verbose output\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    output_key=\"answer\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Invoke with inputs\n",
    "response = llm_chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"content\": content\n",
    "})\n",
    "\n",
    "# 7. Output the answer\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Few-shot Classification Example\n",
    "Demonstrate few-shot learning by providing an example and then classifying a new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T15:57:02.360965Z",
     "iopub.status.busy": "2025-04-17T15:57:02.360267Z",
     "iopub.status.idle": "2025-04-17T15:57:02.887573Z",
     "shell.execute_reply": "2025-04-17T15:57:02.886730Z",
     "shell.execute_reply.started": "2025-04-17T15:57:02.360936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 1. Example for few-shot guidance\n",
    "example_text = \"\"\"\n",
    "Last week's book fair was a delightful gathering of authors and readers, featuring discussions and book signings.\n",
    "\"\"\"\n",
    "example_category = \"Literature\"\n",
    "\n",
    "# 2. Text to classify\n",
    "text = \"\"\"\n",
    "The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Category options\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "# 4. Prompt template for few-shot classification\n",
    "template = \"\"\"\n",
    "Example:\n",
    "Text: {example_text}\n",
    "Category: {example_category}\n",
    "\n",
    "Now, classify the following text into one of the specified categories:\n",
    "{categories}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 5. Initialize OpenAI model via LangChain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=50,\n",
    "    openai_api_key=\"your-openai-api-key\"  # Replace with your OpenAI API key\n",
    ")\n",
    "\n",
    "# 6. Build the chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, output_key=\"category\")\n",
    "\n",
    "# 7. Run the chain with input variables\n",
    "response = llm_chain.invoke({\n",
    "    \"example_text\": example_text,\n",
    "    \"example_category\": example_category,\n",
    "    \"categories\": categories,\n",
    "    \"text\": text\n",
    "})\n",
    "\n",
    "# 8. Output the result\n",
    "print(response[\"category\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. (Optional) Add Your Own Experiments\n",
    "Use this cell for your own prompt engineering or LLM experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
