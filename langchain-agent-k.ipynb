{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries\n",
    "\n",
    "Install all necessary libraries for LangChain, OpenAI, ChromaDB, PDF parsing, and experimental tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T13:37:51.751105Z",
     "iopub.status.busy": "2025-04-20T13:37:51.750765Z",
     "iopub.status.idle": "2025-04-20T13:38:38.748465Z",
     "shell.execute_reply": "2025-04-20T13:38:38.747523Z",
     "shell.execute_reply.started": "2025-04-20T13:37:51.751074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.35)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.1)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.26.4->langchain) (2.4.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.26.4->langchain) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.26.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=7dfe55bf16fba25afd918fe71e2c37c03928be9c5e00d640082b5e6a3a321e82\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, opentelemetry-util-http, mmh3, humanfriendly, httpx-sse, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, openai, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, opentelemetry-instrumentation-fastapi, langchain, onnxruntime, chroma-hnswlib, langchain-community, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.16.0\n",
      "    Uninstalling opentelemetry-api-1.16.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.16.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.61.1\n",
      "    Uninstalling openai-1.61.1:\n",
      "      Successfully uninstalled openai-1.61.1\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.16.0\n",
      "    Uninstalling opentelemetry-sdk-1.16.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.35\n",
      "    Uninstalling langchain-core-0.3.35:\n",
      "      Successfully uninstalled langchain-core-0.3.35\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.6\n",
      "    Uninstalling langchain-text-splitters-0.3.6:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.18\n",
      "    Uninstalling langchain-0.3.18:\n",
      "      Successfully uninstalled langchain-0.3.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-32.0.1 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.54 langchain-openai-0.3.14 langchain-text-splitters-0.3.8 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.1 openai-1.75.0 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 posthog-3.25.0 protobuf-5.29.4 pydantic-settings-2.9.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-core langchain-community langchain-openai openai chromadb pypdf langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "Import all core libraries for OpenAI integration, LangChain LLMs, prompts, message types, output parsers, document loaders, text splitters, vector stores, retrieval, and Pydantic for structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:41:50.432766Z",
     "iopub.status.busy": "2025-04-20T13:41:50.432036Z",
     "iopub.status.idle": "2025-04-20T13:41:50.439141Z",
     "shell.execute_reply": "2025-04-20T13:41:50.438039Z",
     "shell.execute_reply.started": "2025-04-20T13:41:50.432739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OpenAI SDK\n",
    "from openai import OpenAI\n",
    "\n",
    "# LangChain LLM + Embedding integration\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# LangChain Core Building Blocks\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, FewShotPromptTemplate\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "\n",
    "# Text splitting\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Retrieval\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "\n",
    "# Pydantic for structured parsing\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API Key and Define a Simple Generate Function\n",
    "\n",
    "Set your OpenAI API key, initialize the OpenAI client, and define a helper function to generate completions using the OpenAI chat API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:41:55.129840Z",
     "iopub.status.busy": "2025-04-20T13:41:55.129467Z",
     "iopub.status.idle": "2025-04-20T13:41:55.200848Z",
     "shell.execute_reply": "2025-04-20T13:41:55.200092Z",
     "shell.execute_reply.started": "2025-04-20T13:41:55.129811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "open_api_key = \"Your_API_Key\"\n",
    "max_tokens=256\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key= open_api_key)\n",
    "\n",
    "\n",
    "# Define a generate() method to mimic model.generate()\n",
    "def generate(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'results': [{\n",
    "            'generated_text': response.choices[0].message.content\n",
    "        }]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Simple Completion\n",
    "\n",
    "Use the `generate` function to get a completion from the OpenAI API for a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:41:57.353671Z",
     "iopub.status.busy": "2025-04-20T13:41:57.353070Z",
     "iopub.status.idle": "2025-04-20T13:41:57.985129Z",
     "shell.execute_reply": "2025-04-20T13:41:57.984267Z",
     "shell.execute_reply.started": "2025-04-20T13:41:57.353646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm here to help! It seems like your message got cut off. How can I assist you with your sales meeting today?\n"
     ]
    }
   ],
   "source": [
    "msg = generate(\"In today's sales meeting, we \")\n",
    "print(msg['results'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Use a LangChain LLM Wrapper\n",
    "\n",
    "Wrap the OpenAI chat model with LangChain's `ChatOpenAI` class and invoke it directly for a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T14:09:52.224529Z",
     "iopub.status.busy": "2025-04-20T14:09:52.224252Z",
     "iopub.status.idle": "2025-04-20T14:09:52.827665Z",
     "shell.execute_reply": "2025-04-20T14:09:52.826767Z",
     "shell.execute_reply.started": "2025-04-20T14:09:52.224511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog is often considered man's best friend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an OpenAI LLM wrapper\n",
    "openai_llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.5,\n",
    "    openai_api_key= open_api_key,\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "# Call it like before\n",
    "response = openai_llm.invoke(\"Who is man's best friend?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Chat Messages Example\n",
    "\n",
    "Demonstrate how to use structured chat messages with system and human roles for more controlled conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:03.694762Z",
     "iopub.status.busy": "2025-04-20T13:42:03.693855Z",
     "iopub.status.idle": "2025-04-20T13:42:04.291615Z",
     "shell.execute_reply": "2025-04-20T13:42:04.290687Z",
     "shell.execute_reply.started": "2025-04-20T13:42:03.694729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should read \"The Girl with the Dragon Tattoo\" by Stieg Larsson for a gripping and complex mystery story.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Structured chat messages (same as Watsonx)\n",
    "msg = openai_llm.invoke([\n",
    "    SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence\"),\n",
    "    HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "])\n",
    "\n",
    "# Print the reply\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Structured Chat Example\n",
    "\n",
    "Show a multi-turn conversation with system, human, and AI messages to simulate a dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:06.076015Z",
     "iopub.status.busy": "2025-04-20T13:42:06.075598Z",
     "iopub.status.idle": "2025-04-20T13:42:06.748772Z",
     "shell.execute_reply": "2025-04-20T13:42:06.747897Z",
     "shell.execute_reply.started": "2025-04-20T13:42:06.075975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's recommended to attend CrossFit classes 3-5 times per week for optimal results.\n"
     ]
    }
   ],
   "source": [
    "# Structured chat messages (same as Watsonx)\n",
    "msg = openai_llm.invoke([\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "])\n",
    "\n",
    "# Print the reply\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Human Message Example\n",
    "\n",
    "Send a single human message to the LLM and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:09.660567Z",
     "iopub.status.busy": "2025-04-20T13:42:09.660265Z",
     "iopub.status.idle": "2025-04-20T13:42:10.085828Z",
     "shell.execute_reply": "2025-04-20T13:42:10.085026Z",
     "shell.execute_reply.started": "2025-04-20T13:42:09.660543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July\n"
     ]
    }
   ],
   "source": [
    "# Structured chat messages (same as Watsonx)\n",
    "msg = openai_llm.invoke([\n",
    "        HumanMessage(content=\"What month follows June?\")\n",
    "])\n",
    "\n",
    "# Print the reply\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template Example\n",
    "\n",
    "Create a prompt template with placeholders and render it with input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:11.973422Z",
     "iopub.status.busy": "2025-04-20T13:42:11.973117Z",
     "iopub.status.idle": "2025-04-20T13:42:11.980859Z",
     "shell.execute_reply": "2025-04-20T13:42:11.979988Z",
     "shell.execute_reply.started": "2025-04-20T13:42:11.973398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me one funny joke about cats')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Template Example\n",
    "\n",
    "Use a chat prompt template to structure a conversation and render it with input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:14.158777Z",
     "iopub.status.busy": "2025-04-20T13:42:14.158447Z",
     "iopub.status.idle": "2025-04-20T13:42:14.167475Z",
     "shell.execute_reply": "2025-04-20T13:42:14.166505Z",
     "shell.execute_reply.started": "2025-04-20T13:42:14.158752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt with Message Placeholders\n",
    "\n",
    "Demonstrate how to use message placeholders in chat prompts for dynamic message insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:16.229684Z",
     "iopub.status.busy": "2025-04-20T13:42:16.229362Z",
     "iopub.status.idle": "2025-04-20T13:42:16.238295Z",
     "shell.execute_reply": "2025-04-20T13:42:16.237073Z",
     "shell.execute_reply.started": "2025-04-20T13:42:16.229659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the day after Tuesday?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose and Run a LangChain Chain\n",
    "\n",
    "Combine a prompt and LLM into a chain and run it with input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:18.285629Z",
     "iopub.status.busy": "2025-04-20T13:42:18.285311Z",
     "iopub.status.idle": "2025-04-20T13:42:18.795242Z",
     "shell.execute_reply": "2025-04-20T13:42:18.794285Z",
     "shell.execute_reply.started": "2025-04-20T13:42:18.285601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The day after Tuesday is Wednesday.\n"
     ]
    }
   ],
   "source": [
    "# Compose the chain\n",
    "chain = prompt | openai_llm\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke(input=input_)\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompt Example\n",
    "\n",
    "Create a few-shot prompt template with example selector for tasks like antonym generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:21.618396Z",
     "iopub.status.busy": "2025-04-20T13:42:21.618075Z",
     "iopub.status.idle": "2025-04-20T13:42:21.625181Z",
     "shell.execute_reply": "2025-04-20T13:42:21.624019Z",
     "shell.execute_reply.started": "2025-04-20T13:42:21.618371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,  # The maximum length that the formatted examples should be.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Few-Shot Prompt with Input\n",
    "\n",
    "Render the few-shot prompt with a specific adjective to see the formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:30.335700Z",
     "iopub.status.busy": "2025-04-20T13:42:30.335338Z",
     "iopub.status.idle": "2025-04-20T13:42:30.341443Z",
     "shell.execute_reply": "2025-04-20T13:42:30.340454Z",
     "shell.execute_reply.started": "2025-04-20T13:42:30.335677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: big\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Test just the prompt formatting\n",
    "print(dynamic_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Few-Shot Prompt with Long Input\n",
    "\n",
    "Test the few-shot prompt with a long input string to observe truncation or formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:32.860620Z",
     "iopub.status.busy": "2025-04-20T13:42:32.860308Z",
     "iopub.status.idle": "2025-04-20T13:42:32.865451Z",
     "shell.execute_reply": "2025-04-20T13:42:32.864505Z",
     "shell.execute_reply.started": "2025-04-20T13:42:32.860596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Pydantic Data Structure for Structured Output\n",
    "\n",
    "Define a Pydantic model for a joke with setup and punchline fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:35.131595Z",
     "iopub.status.busy": "2025-04-20T13:42:35.131272Z",
     "iopub.status.idle": "2025-04-20T13:42:35.137585Z",
     "shell.execute_reply": "2025-04-20T13:42:35.136693Z",
     "shell.execute_reply.started": "2025-04-20T13:42:35.131570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain for Structured Output with Pydantic Parsing\n",
    "\n",
    "Set up a chain that prompts the LLM and parses the output into a structured Pydantic object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:39.587565Z",
     "iopub.status.busy": "2025-04-20T13:42:39.587249Z",
     "iopub.status.idle": "2025-04-20T13:42:40.201574Z",
     "shell.execute_reply": "2025-04-20T13:42:40.200620Z",
     "shell.execute_reply.started": "2025-04-20T13:42:39.587541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | openai_llm | output_parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comma-Separated List Output Parsing\n",
    "\n",
    "Use a parser to extract a comma-separated list from the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:42.677272Z",
     "iopub.status.busy": "2025-04-20T13:42:42.676900Z",
     "iopub.status.idle": "2025-04-20T13:42:42.682497Z",
     "shell.execute_reply": "2025-04-20T13:42:42.681520Z",
     "shell.execute_reply.started": "2025-04-20T13:42:42.677250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | openai_llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke Chain for List Output\n",
    "\n",
    "Invoke the chain to get a list of items (e.g., ice cream flavors) from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:46.165087Z",
     "iopub.status.busy": "2025-04-20T13:42:46.164315Z",
     "iopub.status.idle": "2025-04-20T13:42:46.699031Z",
     "shell.execute_reply": "2025-04-20T13:42:46.698123Z",
     "shell.execute_reply.started": "2025-04-20T13:42:46.165058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vanilla',\n",
       " 'Chocolate',\n",
       " 'Strawberry',\n",
       " 'Mint Chocolate Chip',\n",
       " 'Cookies and Cream']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Document with Metadata\n",
    "\n",
    "Create a LangChain Document object with page content and metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:48.790572Z",
     "iopub.status.busy": "2025-04-20T13:42:48.790292Z",
     "iopub.status.idle": "2025-04-20T13:42:48.797779Z",
     "shell.execute_reply": "2025-04-20T13:42:48.796493Z",
     "shell.execute_reply.started": "2025-04-20T13:42:48.790553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'About Python', 'my_document_create_time': 1680013019}, page_content=\"Python is an interpreted high-level general-purpose programming language. \\n                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"About Python\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Document Without Metadata\n",
    "\n",
    "Create a Document object with only page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:52.322520Z",
     "iopub.status.busy": "2025-04-20T13:42:52.322217Z",
     "iopub.status.idle": "2025-04-20T13:42:52.328712Z",
     "shell.execute_reply": "2025-04-20T13:42:52.327633Z",
     "shell.execute_reply.started": "2025-04-20T13:42:52.322496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Python is an interpreted high-level general-purpose programming language. \\n                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language. \n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a PDF Document\n",
    "\n",
    "Use PyPDFLoader to load a PDF from a URL and inspect a specific page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:42:53.244302Z",
     "iopub.status.busy": "2025-04-20T13:42:53.243520Z",
     "iopub.status.idle": "2025-04-20T13:42:55.549587Z",
     "shell.execute_reply": "2025-04-20T13:42:55.548689Z",
     "shell.execute_reply.started": "2025-04-20T13:42:53.244270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2023-12-31T03:50:13+00:00', 'author': 'IEEE', 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Figure 2. An AIMessage illustration \\nC. Prompt Template \\nPrompt templates [10] allow you to structure input for LLMs. \\nThey provide a convenient way to format user inputs and \\nprovide instructions to generate responses. Prompt templates \\nhelp ensure that the LLM understands the desired context and \\nproduces relevant outputs. \\nThe prompt template classes in LangChain are built to \\nmake constructing prompts with dynamic inputs easier. Of \\nthese classes, the simplest is the PromptTemplate. \\nD. Chain \\nChains [11] in LangChain refer to the combination of \\nmultiple components to achieve specific tasks. They provide \\na structured and modular approach to building language \\nmodel applications. By combining different components, you \\ncan create chains that address various u se cases and \\nrequirements. Here are some advantages of using chains: \\n• Modularity: Chains allow you to break down \\ncomplex tasks into smaller, manageable \\ncomponents. Each component can be developed and \\ntested independently, making it easier to maintain \\nand update the application. \\n• Simplification: By combining components into a \\nchain, you can simplify the overall implementation \\nof your application. Chains abstract away the \\ncomplexity of working with individual components, \\nproviding a higher-level interface for developers. \\n• Debugging: When an issue arises in your \\napplication, chains can help pinpoint the \\nproblematic component. By isolating the chain and \\ntesting each component individually, you can \\nidentify and troubleshoot any errors or unexpected \\nbehavior. \\n• Maintenance: Chains make it easier to update or \\nreplace specific components without affecting the \\nentire application. If a new version of a component \\nbecomes available or if you want to switch to a \\ndiffer. \\nTo build a chain, you simply combine the desired components \\nin the order they should be executed. Each component in the \\nchain takes the output of the previous component as input, \\nallowing for a seamless flow of data and interaction with the \\nlanguage model. \\nE. Memory  \\nThe ability to remember prior exchanges conversation is \\nreferred to as memory  [12]. LangChain includes several \\nprograms for increasing system memory. These utilities can \\nbe used independently or as a part of a chain.  We call this \\nability to store information about past interactions \"memory\". \\nLangChain provides a lot of utilities for adding memory to a \\nsystem. These utilities can be used by themselves or \\nincorporated seamlessly into a chain. \\nA memory system must support two fundamental \\nactions: reading and writing. Remember that each chain has \\nsome fundamental execution mechanism that requires \\nspecific inputs. Some of these inputs are provided directly by \\nthe user, while others may be retrieve d from memory. In a \\nsingle run, a chain will interact with its memory system twice. \\n1. A chain will READ from its memory system and \\naugment the user inputs AFTER receiving the initial \\nuser inputs but BEFORE performing the core logic. \\n2. After running the basic logic but before providing the \\nsolution, a chain will WRITE the current run\\'s inputs \\nand outputs to memory so that they may be referred \\nto in subsequent runs. \\nAny memory system\\'s two primary design decisions are: \\n1. How state is stored ? \\nStoring: List of chat messages: A history of all chat \\nexchanges is behind each memory. Even if not all of \\nthese are immediately used, they must be preserved \\nin some manner. A series of integrations for storing \\nthese conversation messages, ranging from in -\\nmemory lists to persistent databases, is a significant \\ncomponent of the LangChain memory module. \\n2. How state is queried ? \\nQuerying: Data structures and algorithms on top of \\nchat messages: Keeping track of chat messages is a \\nsimple task. What is less obvious are the data \\nstructures and algorithms built on top of chat \\nconversations to provide the most usable view of \\nthose chats. \\nA simple memory system may only return the most \\nrecent messages on each iteration. A slightly more \\ncomplicated memory system may return a brief summary of \\nthe last K messages. A more complex system might extract \\nentities from stored messages and only retur n information \\nabout entities that have been referenced in the current run. \\nThere are numerous sorts of memories. Each has its own set \\nof parameters and return types and is helpful in a variety of \\nsituations.  \\nMemory Types:  \\n• ConversationBufferMemory allows for saving \\nmessages and then extracts the messages in a \\nvariable. \\n• ConversationBufferWindowMemory keeps a list of \\nthe interactions of the conversation over time. It only \\nuses the last K interactions. This can be useful for \\nkeeping a sliding window of the most recent \\ninteractions, so the buffer does not get too large. \\nThe MindGuide chatbot uses conversation buffer memory. \\nThis memory allows for storing messages and then extracts \\nthe messages in a variable. \\nIII. ARCHITETURE \\nIn crafting the architecture of the MindGuide app, each \\nstep is meticulously designed to create a seamless and \\neffective user experience for those seeking mental health \\nsupport. The user interface, built on Streamlit, sets the tone \\nwith a friendly and safe welcome. Users can jump in by typing \\nWelcome! to your therapy session. I\\'m here to listen, \\nsupport, and guide you through any mental health \\nchallenges or concerns you may have. Please feel free \\nto share what\\'s on your mind, and we\\'ll work together \\nto address your needs. Remember, this is a safe and \\nconfidential space for you to express y ourself. Let\\'s \\nbegin when you\\'re ready.')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "document[2]  # take a look at the page 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Page Content from PDF\n",
    "\n",
    "Print the first 1000 characters of the content from a specific PDF page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:03.345006Z",
     "iopub.status.busy": "2025-04-20T13:43:03.344664Z",
     "iopub.status.idle": "2025-04-20T13:43:03.350228Z",
     "shell.execute_reply": "2025-04-20T13:43:03.349292Z",
     "shell.execute_reply.started": "2025-04-20T13:43:03.344985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these appl\n"
     ]
    }
   ],
   "source": [
    "print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Web Page Content\n",
    "\n",
    "Use WebBaseLoader to load content from a web page and print a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:05.514481Z",
     "iopub.status.busy": "2025-04-20T13:43:05.514187Z",
     "iopub.status.idle": "2025-04-20T13:43:05.904717Z",
     "shell.execute_reply": "2025-04-20T13:43:05.903702Z",
     "shell.execute_reply.started": "2025-04-20T13:43:05.514461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction | ü¶úÔ∏èüîó LangChain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentA newer LangChain version is out! Check out the latest version.IntegrationsAPI referenceLatestLegacyMorePeopleContributingCookbooks3rd party tutorialsYouTubearXivv0.2Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer \n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")\n",
    "\n",
    "web_data = loader.load()\n",
    "\n",
    "print(web_data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Documents into Chunks\n",
    "\n",
    "Split loaded documents into smaller chunks for processing and print the number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:08.379509Z",
     "iopub.status.busy": "2025-04-20T13:43:08.379235Z",
     "iopub.status.idle": "2025-04-20T13:43:08.389946Z",
     "shell.execute_reply": "2025-04-20T13:43:08.388934Z",
     "shell.execute_reply.started": "2025-04-20T13:43:08.379490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")  # define chunk_size which is length of characters, and also separator.\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect a Chunk's Content\n",
    "\n",
    "Print the content of a specific chunk from the split documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:10.365689Z",
     "iopub.status.busy": "2025-04-20T13:43:10.365380Z",
     "iopub.status.idle": "2025-04-20T13:43:10.371627Z",
     "shell.execute_reply": "2025-04-20T13:43:10.370628Z",
     "shell.execute_reply.started": "2025-04-20T13:43:10.365665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'individuals seeking guidance and support in these critical areas. \\nMindGuide lever ages the capabilities of LangChain and its \\nChatModels, specifically Chat OpenAI, as the bedrock of its'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5].page_content   # take a look at any chunk's page content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings for Chunks\n",
    "\n",
    "Initialize OpenAI embeddings, truncate text, and generate embeddings for document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:12.303468Z",
     "iopub.status.busy": "2025-04-20T13:43:12.303131Z",
     "iopub.status.idle": "2025-04-20T13:43:14.504088Z",
     "shell.execute_reply": "2025-04-20T13:43:14.503050Z",
     "shell.execute_reply.started": "2025-04-20T13:43:12.303442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated_texts: * cor\n",
      "[-0.003708885982632637, -0.026277778670191765, -0.012813132256269455, -0.020346276462078094, 0.01486269012093544]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize OpenAI embeddings\n",
    "openai_embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", \n",
    "    openai_api_key= open_api_key\n",
    ")\n",
    "\n",
    "texts = [text.page_content for text in chunks]\n",
    "# print(f\"texts: {texts}\")\n",
    "\n",
    "#`TRUNCATE_INPUT_TOKENS: 3`\n",
    "truncated_texts = [\" \".join(text.split()[:3]) for text in texts]\n",
    "print(f\"truncated_texts: {truncated_texts[0][:5]}\")\n",
    "\n",
    "# Get the embeddings\n",
    "embedding_result = openai_embedding.embed_documents(truncated_texts)\n",
    "\n",
    "# Inspect the first embedding vector\n",
    "print(embedding_result[0][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Chroma Vector Store\n",
    "\n",
    "Create a Chroma vector store from the document chunks and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:15.092408Z",
     "iopub.status.busy": "2025-04-20T13:43:15.092096Z",
     "iopub.status.idle": "2025-04-20T13:43:20.089277Z",
     "shell.execute_reply": "2025-04-20T13:43:20.088197Z",
     "shell.execute_reply.started": "2025-04-20T13:43:15.092385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(chunks, openai_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search Example\n",
    "\n",
    "Perform a similarity search in the vector store for a sample query and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:23.850747Z",
     "iopub.status.busy": "2025-04-20T13:43:23.850426Z",
     "iopub.status.idle": "2025-04-20T13:43:24.259604Z",
     "shell.execute_reply": "2025-04-20T13:43:24.258860Z",
     "shell.execute_reply.started": "2025-04-20T13:43:23.850724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "II. LANGCHAIN \n",
      "LangChain, with its open -source essence, emerges as a \n",
      "promising solution, aiming to simplify the complex process of \n",
      "developing applications powered by large language models\n"
     ]
    }
   ],
   "source": [
    "query = \"Langchain\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Retriever from Vector Store\n",
    "\n",
    "Convert the Chroma vector store into a retriever object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:25.852861Z",
     "iopub.status.busy": "2025-04-20T13:43:25.852542Z",
     "iopub.status.idle": "2025-04-20T13:43:25.857103Z",
     "shell.execute_reply": "2025-04-20T13:43:25.856006Z",
     "shell.execute_reply.started": "2025-04-20T13:43:25.852835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke Retriever for Query\n",
    "\n",
    "Use the retriever to find relevant documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:26.652549Z",
     "iopub.status.busy": "2025-04-20T13:43:26.652249Z",
     "iopub.status.idle": "2025-04-20T13:43:26.972507Z",
     "shell.execute_reply": "2025-04-20T13:43:26.971531Z",
     "shell.execute_reply.started": "2025-04-20T13:43:26.652528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Retrieved Document\n",
    "\n",
    "Print the content of the first retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:27.122456Z",
     "iopub.status.busy": "2025-04-20T13:43:27.122072Z",
     "iopub.status.idle": "2025-04-20T13:43:27.128602Z",
     "shell.execute_reply": "2025-04-20T13:43:27.127757Z",
     "shell.execute_reply.started": "2025-04-20T13:43:27.122430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'creationdate': '2023-12-31T03:50:13+00:00', 'page_label': '1', 'total_pages': 6, 'moddate': '2023-12-31T03:52:06+00:00', 'title': 's8329 final', 'author': 'IEEE', 'page': 0, 'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf', 'producer': 'PyPDF', 'creator': 'Microsoft Word'}, page_content='II. LANGCHAIN \\nLangChain, with its open -source essence, emerges as a \\npromising solution, aiming to simplify the complex process of \\ndeveloping applications powered by large language models')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parent Document Retrieval Setup\n",
    "\n",
    "Set up parent and child text splitters, a Chroma vector store, and an in-memory store for parent document retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:29.747422Z",
     "iopub.status.busy": "2025-04-20T13:43:29.747089Z",
     "iopub.status.idle": "2025-04-20T13:43:29.756696Z",
     "shell.execute_reply": "2025-04-20T13:43:29.755428Z",
     "shell.execute_reply.started": "2025-04-20T13:43:29.747398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=openai_embedding\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize ParentDocumentRetriever\n",
    "\n",
    "Create a ParentDocumentRetriever using the vector store, docstore, and splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:30.313183Z",
     "iopub.status.busy": "2025-04-20T13:43:30.312219Z",
     "iopub.status.idle": "2025-04-20T13:43:30.317372Z",
     "shell.execute_reply": "2025-04-20T13:43:30.316481Z",
     "shell.execute_reply.started": "2025-04-20T13:43:30.313151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Documents to Retriever\n",
    "\n",
    "Add loaded documents to the parent retriever's docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:30.780798Z",
     "iopub.status.busy": "2025-04-20T13:43:30.780491Z",
     "iopub.status.idle": "2025-04-20T13:43:32.788785Z",
     "shell.execute_reply": "2025-04-20T13:43:32.787891Z",
     "shell.execute_reply.started": "2025-04-20T13:43:30.780774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever.add_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Number of Stored Parent Documents\n",
    "\n",
    "Count the number of parent documents stored in the in-memory docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:32.791430Z",
     "iopub.status.busy": "2025-04-20T13:43:32.790614Z",
     "iopub.status.idle": "2025-04-20T13:43:32.797069Z",
     "shell.execute_reply": "2025-04-20T13:43:32.796292Z",
     "shell.execute_reply.started": "2025-04-20T13:43:32.791396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search on Sub-Documents\n",
    "\n",
    "Perform a similarity search on the sub-documents in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:32.798281Z",
     "iopub.status.busy": "2025-04-20T13:43:32.798021Z",
     "iopub.status.idle": "2025-04-20T13:43:33.535187Z",
     "shell.execute_reply": "2025-04-20T13:43:33.534316Z",
     "shell.execute_reply.started": "2025-04-20T13:43:32.798262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Sub-Document Content\n",
    "\n",
    "Print the content of a sub-document retrieved from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:35.403724Z",
     "iopub.status.busy": "2025-04-20T13:43:35.403424Z",
     "iopub.status.idle": "2025-04-20T13:43:35.408852Z",
     "shell.execute_reply": "2025-04-20T13:43:35.407914Z",
     "shell.execute_reply.started": "2025-04-20T13:43:35.403701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to\n"
     ]
    }
   ],
   "source": [
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Parent Documents for Query\n",
    "\n",
    "Use the parent retriever to get relevant parent documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:35.882824Z",
     "iopub.status.busy": "2025-04-20T13:43:35.882528Z",
     "iopub.status.idle": "2025-04-20T13:43:36.301894Z",
     "shell.execute_reply": "2025-04-20T13:43:36.301220Z",
     "shell.execute_reply.started": "2025-04-20T13:43:35.882801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Retrieved Parent Document Content\n",
    "\n",
    "Print the content of the first parent document retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:36.541441Z",
     "iopub.status.busy": "2025-04-20T13:43:36.541148Z",
     "iopub.status.idle": "2025-04-20T13:43:36.546099Z",
     "shell.execute_reply": "2025-04-20T13:43:36.545010Z",
     "shell.execute_reply.started": "2025-04-20T13:43:36.541418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain helps us to unlock the ability to harness the \n",
      "LLM’s immense potential in tasks such as document analysis, \n",
      "chatbot development, code analysis, and countless other \n",
      "applications. Whether your desire is to unlock deeper natural \n",
      "language understanding , enhance data, or circumvent \n",
      "language barriers through translation, LangChain is ready to \n",
      "provide the tools and programming support you need to do \n",
      "without it that it is not only difficult but also fresh for you. Its \n",
      "core functionalities encompass: \n",
      "1. Context-Aware Capabilities: LangChain facilitates the \n",
      "development of applications that are inherently \n",
      "context-aware. This means that these applications can \n",
      "connect to a language model and draw from various \n",
      "sources of context, such as prompt instructions, a few-\n",
      "shot examples, or existing content, to ground their \n",
      "responses effectively. \n",
      "2. Reasoning Abilities: LangChain equips applications \n",
      "with the capacity to reason effectively. By relying on a \n",
      "language model, these applications can make informed \n",
      "decisions about how to respond based on the provided \n",
      "context and determine the appropriate actions to take. \n",
      "LangChain offers several key value propositions: \n",
      "Modular Components: It provides abstractions that \n",
      "simplify working with language models, along with a \n",
      "comprehensive collection of implementations for each \n",
      "abstraction. These components are designed to be modular \n",
      "and user -friendly, making them useful whethe r you are \n",
      "utilizing the entire LangChain framework or not. \n",
      "Off-the-Shelf Chains: LangChain offers pre -configured \n",
      "chains, which are structured assemblies of components \n",
      "tailored to accomplish specific high -level tasks. These pre -\n",
      "defined chains streamline the initial setup process and serve as \n",
      "an ideal starting point for your projects. The MindGuide Bot \n",
      "uses below components from LangChain. \n",
      "A. ChatModel \n",
      "Within LangChain, a ChatModel is a specific kind of \n",
      "language model crafted to manage conversational\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up RetrievalQA Chain\n",
    "\n",
    "Create a RetrievalQA chain using the LLM and retriever, and invoke it with a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:39.507447Z",
     "iopub.status.busy": "2025-04-20T13:43:39.507124Z",
     "iopub.status.idle": "2025-04-20T13:43:39.511898Z",
     "shell.execute_reply": "2025-04-20T13:43:39.510987Z",
     "shell.execute_reply.started": "2025-04-20T13:43:39.507418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:39.894654Z",
     "iopub.status.busy": "2025-04-20T13:43:39.894346Z",
     "iopub.status.idle": "2025-04-20T13:43:40.861060Z",
     "shell.execute_reply": "2025-04-20T13:43:40.860083Z",
     "shell.execute_reply.started": "2025-04-20T13:43:39.894623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is this paper discussing?',\n",
       " 'result': 'This paper is discussing the application of recent interventions for mental disorders, specifically focusing on anxiety, depression, and suicidal thoughts, using techniques that have been effective in text processing tasks.'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=openai_llm, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=False)\n",
    "query = \"what is this paper discussing?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import ChatMessageHistory for Conversation Memory\n",
    "\n",
    "Import the ChatMessageHistory class to manage conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:40.862774Z",
     "iopub.status.busy": "2025-04-20T13:43:40.862414Z",
     "iopub.status.idle": "2025-04-20T13:43:40.867434Z",
     "shell.execute_reply": "2025-04-20T13:43:40.866496Z",
     "shell.execute_reply.started": "2025-04-20T13:43:40.862751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Update Conversation History\n",
    "\n",
    "Create a conversation history object and add AI and user messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:40.868915Z",
     "iopub.status.busy": "2025-04-20T13:43:40.868591Z",
     "iopub.status.idle": "2025-04-20T13:43:40.878240Z",
     "shell.execute_reply": "2025-04-20T13:43:40.877089Z",
     "shell.execute_reply.started": "2025-04-20T13:43:40.868891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chat = openai_llm\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Conversation Messages\n",
    "\n",
    "Display the list of messages in the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:43.229643Z",
     "iopub.status.busy": "2025-04-20T13:43:43.229330Z",
     "iopub.status.idle": "2025-04-20T13:43:43.235556Z",
     "shell.execute_reply": "2025-04-20T13:43:43.234830Z",
     "shell.execute_reply.started": "2025-04-20T13:43:43.229619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of France?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke LLM with Conversation History\n",
    "\n",
    "Send the conversation history to the LLM and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:43.619832Z",
     "iopub.status.busy": "2025-04-20T13:43:43.619495Z",
     "iopub.status.idle": "2025-04-20T13:43:44.047575Z",
     "shell.execute_reply": "2025-04-20T13:43:44.046609Z",
     "shell.execute_reply.started": "2025-04-20T13:43:43.619807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 20, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BOPFPhimFAvckTnHzmjHrELpu7BpI', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1198908-7fc2-463b-874f-b124eb973a58-0', usage_metadata={'input_tokens': 20, 'output_tokens': 8, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Conversation Memory and Chain\n",
    "\n",
    "Import ConversationBufferMemory and ConversationChain for managing conversational state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:44.053093Z",
     "iopub.status.busy": "2025-04-20T13:43:44.052734Z",
     "iopub.status.idle": "2025-04-20T13:43:44.057238Z",
     "shell.execute_reply": "2025-04-20T13:43:44.056373Z",
     "shell.execute_reply.started": "2025-04-20T13:43:44.053069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Conversation Chain\n",
    "\n",
    "Set up a conversation chain with memory and verbosity enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:44.487348Z",
     "iopub.status.busy": "2025-04-20T13:43:44.487018Z",
     "iopub.status.idle": "2025-04-20T13:43:44.492193Z",
     "shell.execute_reply": "2025-04-20T13:43:44.491261Z",
     "shell.execute_reply.started": "2025-04-20T13:43:44.487325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=openai_llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a Conversation\n",
    "\n",
    "Invoke the conversation chain with an initial message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:49.367659Z",
     "iopub.status.busy": "2025-04-20T13:43:49.367352Z",
     "iopub.status.idle": "2025-04-20T13:43:50.161103Z",
     "shell.execute_reply": "2025-04-20T13:43:50.159979Z",
     "shell.execute_reply.started": "2025-04-20T13:43:49.367638Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello, I am a little cat. Who are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello, I am a little cat. Who are you?',\n",
       " 'history': '',\n",
       " 'response': 'Hello little cat! I am an artificial intelligence program designed to assist and provide information to users like yourself. I am constantly learning and updating my database to better serve you. How can I help you today?'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Hello, I am a little cat. Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue the Conversation\n",
    "\n",
    "Send a follow-up message to the conversation chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:50.162664Z",
     "iopub.status.busy": "2025-04-20T13:43:50.162413Z",
     "iopub.status.idle": "2025-04-20T13:43:50.930364Z",
     "shell.execute_reply": "2025-04-20T13:43:50.929412Z",
     "shell.execute_reply.started": "2025-04-20T13:43:50.162642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, I am a little cat. Who are you?\n",
      "AI: Hello little cat! I am an artificial intelligence program designed to assist and provide information to users like yourself. I am constantly learning and updating my database to better serve you. How can I help you today?\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What can you do?',\n",
       " 'history': 'Human: Hello, I am a little cat. Who are you?\\nAI: Hello little cat! I am an artificial intelligence program designed to assist and provide information to users like yourself. I am constantly learning and updating my database to better serve you. How can I help you today?',\n",
       " 'response': 'I can provide information on a wide range of topics, answer questions, assist with tasks like scheduling appointments or setting reminders, offer recommendations based on your preferences, and even engage in casual conversation like we are doing now. Just let me know what you need help with!'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SequentialChain for Multi-Step Workflows\n",
    "\n",
    "Import SequentialChain to build multi-step LLM workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:52.839034Z",
     "iopub.status.busy": "2025-04-20T13:43:52.838670Z",
     "iopub.status.idle": "2025-04-20T13:43:52.844429Z",
     "shell.execute_reply": "2025-04-20T13:43:52.842983Z",
     "shell.execute_reply.started": "2025-04-20T13:43:52.839001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Sequential chain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Sequential Chain for Movie Recommendation\n",
    "\n",
    "Create a two-step chain: recommend a classic movie by location, then describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:53.202607Z",
     "iopub.status.busy": "2025-04-20T13:43:53.202309Z",
     "iopub.status.idle": "2025-04-20T13:43:55.221890Z",
     "shell.execute_reply": "2025-04-20T13:43:55.221014Z",
     "shell.execute_reply.started": "2025-04-20T13:43:53.202585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Movie: \"The Last Samurai\" (2003) - This epic historical drama film is set in Kyoto, Japan during the 19th century and follows the story of an American military officer who is captured by samurai warriors and learns the way of the samurai. The film showcases the beautiful landscapes and traditional architecture of Kyoto, making it a classic movie from the area.\n",
      "Description: \"The Last Samurai\" is a visually stunning film that captures the essence of traditional Japanese culture and history. The breathtaking landscapes of Kyoto serve as a backdrop for the epic story of a foreign soldier who finds himself immersed in the world of the samurai. The film beautifully portrays the intricate details of Japanese architecture and the rich cultural heritage of the region. With its compelling storyline and captivating visuals, \"The Last Samurai\" is a classic depiction of honor, loyalty, and the clash of civilizations.\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1: Recommend a classic movie\n",
    "template = \"\"\"Your job is to come up with a classic movie from the area the user suggests.\n",
    "Location: {location}\n",
    "\n",
    "YOUR RESPONSE:\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['location'])\n",
    "\n",
    "# First chain\n",
    "location_chain = LLMChain(llm=openai_llm, prompt=prompt_template, output_key='movie')\n",
    "\n",
    "# Prompt 2: Describe the movie\n",
    "desc_template = \"\"\"You are an art critic. Describe the classical \"{movie}\" in a short paragraph.\"\"\"\n",
    "desc_prompt = PromptTemplate(template=desc_template, input_variables=['movie'])\n",
    "\n",
    "# Second chain\n",
    "desc_chain = LLMChain(llm=openai_llm, prompt=desc_prompt, output_key='description')\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[location_chain, desc_chain],\n",
    "    input_variables=['location'],\n",
    "    output_variables=['movie', 'description'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run it\n",
    "result = overall_chain.invoke({'location': 'Kyoto'})\n",
    "print(\"Movie:\", result['movie'])\n",
    "print(\"Description:\", result['description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SequentialChain Again (Redundant)\n",
    "\n",
    "Re-import SequentialChain (already imported above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:43:56.802403Z",
     "iopub.status.busy": "2025-04-20T13:43:56.802113Z",
     "iopub.status.idle": "2025-04-20T13:43:56.807248Z",
     "shell.execute_reply": "2025-04-20T13:43:56.806059Z",
     "shell.execute_reply.started": "2025-04-20T13:43:56.802382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chain for Classic Dish Recommendation\n",
    "\n",
    "Create a prompt and chain to recommend a classic dish by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:48:37.923549Z",
     "iopub.status.busy": "2025-04-20T13:48:37.923239Z",
     "iopub.status.idle": "2025-04-20T13:48:37.929115Z",
     "shell.execute_reply": "2025-04-20T13:48:37.928159Z",
     "shell.execute_reply.started": "2025-04-20T13:48:37.923527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "                {location}\n",
    "                \n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['location'])\n",
    "\n",
    "# chain 1\n",
    "location_chain = LLMChain(llm=openai_llm, prompt=prompt_template, output_key='meal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chain for Recipe Generation\n",
    "\n",
    "Create a prompt and chain to generate a recipe for a given meal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:47:18.375683Z",
     "iopub.status.busy": "2025-04-20T13:47:18.375344Z",
     "iopub.status.idle": "2025-04-20T13:47:18.381902Z",
     "shell.execute_reply": "2025-04-20T13:47:18.380898Z",
     "shell.execute_reply.started": "2025-04-20T13:47:18.375658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['meal'])\n",
    "\n",
    "# chain 2\n",
    "dish_chain = LLMChain(llm=openai_llm, prompt=prompt_template, output_key='recipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chain for Cooking Time Estimation\n",
    "\n",
    "Create a prompt and chain to estimate cooking time for a recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:47:20.121513Z",
     "iopub.status.busy": "2025-04-20T13:47:20.121218Z",
     "iopub.status.idle": "2025-04-20T13:47:20.126403Z",
     "shell.execute_reply": "2025-04-20T13:47:20.125569Z",
     "shell.execute_reply.started": "2025-04-20T13:47:20.121484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['recipe'])\n",
    "\n",
    "# chain 3\n",
    "recipe_chain = LLMChain(llm=openai_llm, prompt=prompt_template, output_key='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run a Multi-Step Chain for Meals\n",
    "\n",
    "Combine the dish, recipe, and time chains into a sequential workflow and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:47:20.451606Z",
     "iopub.status.busy": "2025-04-20T13:47:20.451299Z",
     "iopub.status.idle": "2025-04-20T13:47:20.456689Z",
     "shell.execute_reply": "2025-04-20T13:47:20.455655Z",
     "shell.execute_reply.started": "2025-04-20T13:47:20.451581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# overall chain\n",
    "overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],\n",
    "                                      input_variables=['location'],\n",
    "                                      output_variables=['meal', 'recipe', 'time'],\n",
    "                                      verbose= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pprint for Pretty Printing\n",
    "\n",
    "Import the pprint module to display results in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:47:25.060606Z",
     "iopub.status.busy": "2025-04-20T13:47:25.060311Z",
     "iopub.status.idle": "2025-04-20T13:47:25.064811Z",
     "shell.execute_reply": "2025-04-20T13:47:25.063925Z",
     "shell.execute_reply.started": "2025-04-20T13:47:25.060584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Multi-Step Meal Chain and Print Results\n",
    "\n",
    "Invoke the meal chain with a location and print the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:47:25.390513Z",
     "iopub.status.busy": "2025-04-20T13:47:25.390211Z",
     "iopub.status.idle": "2025-04-20T13:47:29.139050Z",
     "shell.execute_reply": "2025-04-20T13:47:29.138188Z",
     "shell.execute_reply.started": "2025-04-20T13:47:25.390490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'location': 'China',\n",
      " 'meal': 'One classic dish from China is Peking duck. This dish originated in '\n",
      "         'Beijing and is known for its crispy skin and tender meat. The duck '\n",
      "         'is typically roasted and served with pancakes, scallions, cucumber, '\n",
      "         'and hoisin sauce. It is a popular dish in Chinese cuisine and is '\n",
      "         'often served at special occasions and celebrations.',\n",
      " 'recipe': 'Ingredients:\\n'\n",
      "           '- 1 whole duck\\n'\n",
      "           '- Salt\\n'\n",
      "           '- 1 tablespoon honey\\n'\n",
      "           '- 1 tablespoon soy sauce\\n'\n",
      "           '- 1 tablespoon vinegar\\n'\n",
      "           '- Hoisin sauce\\n'\n",
      "           '- Thin pancakes\\n'\n",
      "           '- Sliced scallions\\n'\n",
      "           '- Sliced cucumber\\n'\n",
      "           '\\n'\n",
      "           'Instructions:\\n'\n",
      "           '1. Preheat the oven to 375°F.\\n'\n",
      "           '2. Clean the duck and pat it dry. Rub salt all over the skin.\\n'\n",
      "           '3. In a small bowl, mix honey, soy sauce, and vinegar. Brush the '\n",
      "           'mixture all over the duck.\\n'\n",
      "           '4. Place the duck on a roasting rack in a roasting pan and roast '\n",
      "           'in the oven for 2-3 hours, until the skin is crispy and golden '\n",
      "           'brown.\\n'\n",
      "           '5. Let the duck cool slightly, then carve it into thin slices.\\n'\n",
      "           '6. Serve the duck with pancakes, hoisin sauce, sliced scallions, '\n",
      "           'and sliced cucumber. Enjoy your homemade Peking duck!',\n",
      " 'time': 'Based on the recipe instructions, it will take approximately 2-3 '\n",
      "         'hours to cook the Peking duck in the oven.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(overall_chain.invoke(input={'location':'China'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Summarization Chain\n",
    "\n",
    "Import the summarization chain from LangChain for document summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:48:19.691408Z",
     "iopub.status.busy": "2025-04-20T13:48:19.691125Z",
     "iopub.status.idle": "2025-04-20T13:48:19.698454Z",
     "shell.execute_reply": "2025-04-20T13:48:19.697743Z",
     "shell.execute_reply.started": "2025-04-20T13:48:19.691389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Summarization Chain on Web Data\n",
    "\n",
    "Use the summarization chain to summarize loaded web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:49:27.223294Z",
     "iopub.status.busy": "2025-04-20T13:49:27.222926Z",
     "iopub.status.idle": "2025-04-20T13:49:28.662931Z",
     "shell.execute_reply": "2025-04-20T13:49:28.661698Z",
     "shell.execute_reply.started": "2025-04-20T13:49:27.223271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(openai_llm, chain_type=\"stuff\", verbose=False)\n",
    "response = chain.invoke(web_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Summarization Output\n",
    "\n",
    "Print the output text from the summarization chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T13:49:31.380783Z",
     "iopub.status.busy": "2025-04-20T13:49:31.380191Z",
     "iopub.status.idle": "2025-04-20T13:49:31.385382Z",
     "shell.execute_reply": "2025-04-20T13:49:31.384332Z",
     "shell.execute_reply.started": "2025-04-20T13:49:31.380746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies the development, productionization, and deployment stages of LLM applications, offering open-source building blocks, components, and third-party integrations. The framework includes libraries such as LangGraph for building stateful agents, LangServe for deploying chains as REST APIs, and LangSmith for debugging and monitoring LLM applications. The documentation provides tutorials, how-to guides, API references, and conceptual guides for developers using the LangChain framework.\n"
     ]
    }
   ],
   "source": [
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Empty Cell)\n",
    "This cell is intentionally left blank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python REPL Tool\n",
    "\n",
    "Import the Python REPL tool for executing Python code within LangChain agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Python REPL\n",
    "\n",
    "Create an instance of the Python REPL for code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Python Code in REPL\n",
    "\n",
    "Execute a simple Python code snippet using the Python REPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl.run(\"a = 3; b = 1; print(a+b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import PythonREPLTool\n",
    "\n",
    "Import the PythonREPLTool for use in LangChain agent toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Python REPL Tool List\n",
    "\n",
    "Create a list of tools including the Python REPL tool for agent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [PythonREPLTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Agent and Tool Classes\n",
    "\n",
    "Import classes for creating and running LangChain agents with custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Python REPL Agent\n",
    "\n",
    "Set up a custom agent with a Python REPL tool, define instructions, and run an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom instructions\n",
    "instructions = \"\"\"You are an agent designed to write and execute Python code to answer questions.\n",
    "You have access to a Python REPL, which you can use to execute Python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template = \"\"\"You are an agent designed to write and execute Python code to answer questions.\n",
    "You have access to a Python REPL, which you can use to execute Python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "\n",
    "Begin!\n",
    "New input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\", \"tools\", \"tool_names\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Tool setup\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Python_REPL\",\n",
    "        func=lambda x: python_repl.run(clean_python_input(x)),\n",
    "        description=\"A Python shell. Use this to execute Python code.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(openai_llm, tools, prompt)\n",
    "\n",
    "# Create agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "# Run example\n",
    "response = agent_executor.invoke(input={\"input\": \"What is the 3rd fibonacci number?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
